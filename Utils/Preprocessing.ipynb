{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and preliminary operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione pacchetti\n",
    "# %pip install nltk\n",
    "# %pip install gensim\n",
    "# %pip install simplemma\n",
    "# %pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import sys\n",
    "import io \n",
    "import nltk # Natural Language Processing package\n",
    "\n",
    "# Stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('italian'))\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('italian')\n",
    "# Valutare di aggiungere eventuali forme arcariche delle stopwords italiane, o se c'è necessità di estendere questa lista\n",
    "\n",
    "# Bigrammi\n",
    "from gensim.models import Phrases  \n",
    "\n",
    "# Per dividere il testo in frasi in base ai punti\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Per tokenizzare\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Position tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "import simplemma # altro pacchetto per lemmatizzare\n",
    "\n",
    "import spacy # altro pacchetto per lemmatizzare\n",
    "import it_core_news_lg\n",
    "\n",
    "# Contatore parole uniche\n",
    "from collections import Counter\n",
    "\n",
    "# Per esplorare risultati\n",
    "import random\n",
    "\n",
    "import unicodedata # Pulizia dei caratteri speciali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista stopwords ulteriore (no forme arcaiche presenti)\n",
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-it/master/stopwords-it.txt'\n",
    "resp = requests.get(url)\n",
    "\n",
    "with open(\"stopwords_ext.txt\", 'w') as file:\n",
    "    file.write(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text): # Normalizzo tutte le parole togliendo caratteri speciali tipo accenti\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def append_new_line(file_name, text_to_append):\n",
    "\n",
    "    # Apro il file in 'append' e 'lettura'\n",
    "    with open(file_name, \"a+\") as file:\n",
    "        file.seek(0)\n",
    "\n",
    "        # Se il file non è vuoto vado a capo\n",
    "        if len(file.read(100)) > 0:\n",
    "            file.write(\"\\n\")\n",
    "        # Inserisco la stringa\n",
    "        file.write(text_to_append)\n",
    "with open ('stopwords_ext.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        append_new_line('Output/stopwords_ext_clean.txt', strip_accents(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estensione della lista delle stopwords (al momento non sono presenti forme arcaiche)\n",
    "stopwords_ext = []\n",
    "with open('Output/stopwords_ext_clean.txt') as f:\n",
    "    for line in f:\n",
    "         stopwords_ext.append(line)\n",
    "stopwords_ext = [s.replace('\\n', '') for s in stopwords_ext]\n",
    "\n",
    "# Unisco la lista di stopwords di nltk con quella estratta dal txt\n",
    "stopwords = list(nltk_stopwords)\n",
    "stopwords.extend(x for x in stopwords_ext if x not in stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrammi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import string\n",
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b873a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences,\n",
    "                      min_count=5,\n",
    "                      threshold=10,\n",
    "                      progress_per=1000)\n",
    "    return Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ca24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questa funzione ricostruisce la frase con i bigrammi\n",
    "def sentence_to_bi_grams(phrases_model, sentence):\n",
    "    return ' '.join(phrases_model[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_preproc(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/autori/')\n",
    "\n",
    "# Unisco tutti i corpus degli autori in un unico corpus\n",
    "path = 'C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/autori/'\n",
    "filenames =  listdir(path)\n",
    "\n",
    "with open(path + 'total_corpus.txt', 'w', encoding='utf-8') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding='utf-8') as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "# Eseguo il simple pre-processing sull'intero corpus\n",
    "with open('full_corpus.txt', encoding='utf-8') as read_file:\n",
    "    sentences = [simple_preproc(k).lower().split() for k in read_file.readlines()]\n",
    "full_corpus_models = build_phrases(sentences)\n",
    "full_corpus_models.save('bigrams_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing/periodi_storici')\n",
    "    \n",
    "with open('tardo_medioevo_sentences_nl.txt', encoding='utf-8') as read_file:\n",
    "    sentences = [simple_preproc(k).lower().split() for k in read_file.readlines()]\n",
    "phrases_model = build_phrases(sentences)\n",
    "phrases_model.save('phrases_model_tardo_medioevo_nl.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing considerando i bigrammi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4358ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adatto la funzione di preprocessing per inserire anche i bigrammi\n",
    "\n",
    "def preprocessing_w_bigrams(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze ecc..\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frasi, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    filtered_sentence = [w for w in noalfa_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Valuto i bigrammi\n",
    "    bigram_sentence = sentence_to_bi_grams(phrases_model, filtered_sentence)\n",
    "    bigram_sentence = bigram_sentence.split() # Rimetto tutto in token\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if bigram_sentence:\n",
    "      filtered_sentences.append(bigram_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing senza lemmittazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze,...\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frase, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    filtered_sentence = [w for w in noalfa_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if filtered_sentence:\n",
    "      filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing con lemmatizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lem(file_name):\n",
    "\n",
    "    output = \"\"\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():  # Rimuovo linee vuote\n",
    "                output += line\n",
    "\n",
    "    # Divido il testo in frasi, basandomi sui punti\n",
    "    output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "    # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n",
    "    # output_sentences = output_sentences[:-]\n",
    "\n",
    "    filtered_sentences = []\n",
    "    # 'Pulisco' ogni frasi, una alla volta\n",
    "    for sentence in output_sentences:\n",
    "        # Metto tutto in lower case\n",
    "        lower_sentence = sentence.lower()\n",
    "        # Rimuovo caratteri non alfa numerici\n",
    "        noalfa_sentence = [w for w in word_tokenize(\n",
    "            lower_sentence) if (w.isalpha() == True)]\n",
    "        # Lemmatize\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(\n",
    "            w, get_wordnet_pos(w)) for w in noalfa_sentence]\n",
    "        # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "        filtered_sentence = [w for w in lemmatized_sentence if (\n",
    "            (w not in stops) and (len(w) > 1))]\n",
    "        # Ricostruisco la lista con le frasi 'pulite'\n",
    "        if filtered_sentence:\n",
    "            filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing con lemmatizzazione w/Simplemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplemma\n",
    "\n",
    "# Preprocessing (CON lemmatization w/ SIMPLEMMA)\n",
    "\n",
    "\n",
    "def preprocessing_simplemma(file_name):\n",
    "\n",
    "    output = \"\"\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():  # Rimuovo linee vuote\n",
    "                output += line\n",
    "\n",
    "    # Divido il testo in frasi, basandomi sui punti\n",
    "    output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "    # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n",
    "    # output_sentences = output_sentences[:-]\n",
    "\n",
    "    filtered_sentences = []\n",
    "    # 'Pulisco' ogni frasi, una alla volta\n",
    "    for sentence in output_sentences:\n",
    "        # Metto tutto in lower case\n",
    "        lower_sentence = sentence.lower()\n",
    "        # Rimuovo caratteri non alfa numerici\n",
    "        noalfa_sentence = [w for w in word_tokenize(\n",
    "            lower_sentence) if (w.isalpha() == True)]\n",
    "        # Lemmatize\n",
    "        lemmatized_sentence = [simplemma.lemmatize(\n",
    "            w, lang='it') for w in noalfa_sentence]\n",
    "        # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "        filtered_sentence = [w for w in lemmatized_sentence if (\n",
    "            (w not in stops) and (len(w) > 1))]\n",
    "        # Ricostruisco la lista con le frasi 'pulite'\n",
    "        if filtered_sentence:\n",
    "            filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    return filtered_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing con lemmatizzazione w/Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import it_core_news_lg\n",
    "\n",
    "nlp = it_core_news_lg.load()\n",
    "\n",
    "# Preprocessing (CON lemmatization w/ SPACY). In questo caso ho rimosso le stopwords prima della lemmatizzazione, va provato\n",
    "# con tutti a fare così\n",
    "\n",
    "def preprocessing_spacy(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frasi, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    nostop_sentence = [w for w in noalfa_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Lemmatize\n",
    "    lemmatized_sentence = \" \".join([w for w in nostop_sentence])\n",
    "    doc = nlp(lemmatized_sentence)\n",
    "    filtered_sentence = [w.lemma_ for w in doc]\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if filtered_sentence:\n",
    "      filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione delle varie funzioni di pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per esplorare i risultati\n",
    "def checkresults(sentences):\n",
    "  words = []\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "      words.append(sentences[i][j])\n",
    "\n",
    "  # Numero totale di parole\n",
    "  print('Parole: ' + str(len(words)))\n",
    "\n",
    "  # Numero di parole uniche\n",
    "  word_count = Counter(words)\n",
    "  keys = word_count.keys()\n",
    "  print('Parole uniche: '+ str(len(keys)))\n",
    "\n",
    "  # Parole più comuni\n",
    "  print('Parole più comuni:')\n",
    "  print(word_count.most_common(10))\n",
    "\n",
    "  # Stampo alcune frasi random\n",
    "  print('Frasi random:')\n",
    "  randomlist = random.sample(range(0, len(sentences)), 5)\n",
    "  for i in randomlist:\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savesentences(filename, sentences):\n",
    "  os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing')\n",
    "  with open(filename + '.txt', 'w', encoding='utf-8') as fp:\n",
    "    for sentence in sentences:\n",
    "      fp.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Senza lemmatizzazione\n",
    "def savesentencesnl(filename, sentences):\n",
    "  os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing')\n",
    "  with open(filename + '_nl.txt', 'w', encoding='utf-8') as fp:\n",
    "    for sentence in sentences:\n",
    "      fp.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1000_1250_Pieno Medievo.txt', '1250_1454_Tardo_Medioevo.txt', '1454_1559_Rinascimento.txt', '1559_1707_Seicento.txt', '1707_1796_Settecento.txt', '1796_1814_Periodo_napoleonico.txt', '1814_1860_Risorgimento.txt', \"1861_1914_L'Italia_liberale.txt\", \"1915_1922_L'Italia_nella_Prima_Guerra_Mondiale_e_dopoguerra.txt\", '1922_1945_Ventennio_Fascista_Seconda_Guerra_Mondiale.txt', '1945_presente_La_Repubblica_Italiana.txt']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "directory = 'C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/periodi_storici'\n",
    "files_dir =  listdir(directory)\n",
    "periodi_storici_files = []\n",
    "for names in files_dir:\n",
    "    if names.endswith(\".txt\"):\n",
    "        periodi_storici_files.append(names)\n",
    "print (periodi_storici_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus per periodi storici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senza lemmatizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus 'Pieno medioevo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/periodi_storici')\n",
    "pieno_medioevo_sentences = []\n",
    "pieno_medioevo_sentences = preprocessing(periodi_storici_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 3732\n",
      "Parole uniche: 1758\n",
      "Parole più comuni:\n",
      "[('no', 51), ('sì', 51), ('de', 48), ('donna', 44), ('core', 38), ('né', 35), ('et', 28), ('ben', 26), ('ca', 25), ('gran', 25)]\n",
      "Frasi random:\n",
      "['dunque', 'madonna', 'voglio', 'sofrire', 'far', 'sembianti', 'contrata', 'gente', 'sforza', 'maldire', 'però', 'blasmata', 'diletta', 'dire', 'male', 'bene', 'fiata']\n",
      "['foco', 'inchiuso', 'poi', 'passa', 'difore', 'lostrore', 'sanza', 'far', 'rotura', 'così', 'ochi', 'pass', 'core', 'no', 'persona', 'figura']\n",
      "['gran', 'disio', 'dipinsi', 'pintura', 'bella', 'simigliante', 'quando', 'vio', 'guardo', 'figura', 'par', 'davante', 'crede', 'salvarsi', 'fede', 'ancor', 'veggia', 'inante']\n",
      "['conventi', 'ritenere', 'donaomi', 'gio', 'rimembranza', 'allegramente']\n",
      "['xxvi', 'claro', 'vista', 'ploggia', 'dare', 'scuro', 'rendere', 'clarore', 'foco', 'arzente', 'ghiaccia', 'diventare', 'freda', 'neve', 'rendere', 'calore', 'dolze', 'cose', 'molto', 'amareare', 'de', 'rendere', 'dolzore', 'dui', 'guerreri', 'fina', 'pace', 'stare', 'dui', 'amici', 'nascereci', 'errore']\n"
     ]
    }
   ],
   "source": [
    "checkresults(pieno_medioevo_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "savesentencesnl('pieno_medioevo_sentences', pieno_medioevo_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus tardo medioevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/periodi_storici')\n",
    "tardo_medioevo_sentences = preprocessing(periodi_storici_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 2622917\n",
      "Parole uniche: 139172\n",
      "Parole più comuni:\n",
      "[('de', 27661), ('sì', 16223), ('poi', 11661), ('me', 11227), ('così', 10646), ('ciò', 9992), ('quando', 9814), ('disse', 9420), ('cosa', 9365), ('cioè', 8821)]\n",
      "Frasi random:\n",
      "['gridai', 'alto', 'dio', 'signore', 'render', 'posso', 'tanti', 'benefici', 'ami', 'te', 'core']\n",
      "['però', 'dice', 'ten', 'spalle', 'volte', 'verso', 'damiata', 'aciò', 'mondo', 'quel', 'aver', 'respeto', 'parte', 'mondo']\n",
      "['el', 'sire', 'iddio', 'potenza', 'immergerà', 'tutta', 'possa', 'scura', 'fossa', 'lì', 'sempre', 'viso', 'lordo', 'già', 'mai', 'de', 'fia', 'recordo']\n",
      "['cioè', 'tosto', 'pena', 'purgati']\n",
      "['egli', 'fitto', 'capo', 'ponte', 'colonna', 'marmo', 'sotto', 'colonna', 'hae', 'lione', 'marmo', 'sopra', 'altro', 'molto', 'begli', 'grandi', 'ben', 'fatti', 'lungi', 'colonna', 'passo', 'altra', 'né', 'piú', 'né', 'meno', 'fatta', 'due', 'leoni', 'colonna', 'altra', 'chiuso', 'tavole', 'marmo', 'percioché', 'niuno', 'potesse', 'cadere', 'acqua']\n"
     ]
    }
   ],
   "source": [
    "checkresults(tardo_medioevo_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "savesentencesnl('tardo_medioevo_sentences', tardo_medioevo_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Rinascimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/periodi_storici')\n",
    "rinascimento_sentences = preprocessing(periodi_storici_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "savesentencesnl('rinascimento_sentences', rinascimento_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Seicento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/periodi_storici')\n",
    "seicento_sentences = preprocessing(periodi_storici_files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "savesentencesnl('seicento_sentences', seicento_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Settecento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/periodi_storici')\n",
    "settecento_sentences = preprocessing(periodi_storici_files[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "savesentencesnl('settecento_sentences', settecento_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/periodi_storici')\n",
    "La_repubblica_italiana = preprocessing(periodi_storici_files[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "savesentencesnl('La_repubblica_italiana', La_repubblica_italiana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus per fasi letterarie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senza lemmatizzazione (campione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/fasi_letterarie')\n",
    "avanguardie_primo_novecento_senteces = []\n",
    "avanguardie_primo_novecento_senteces = preprocessing('11_avanguardie_primo_novecento.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing/fasi_letterarie')\n",
    "savesentencesnl('avanguardie_primo_novecento_senteces', avanguardie_primo_novecento_senteces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 344934\n",
      "Parole uniche: 50367\n",
      "Parole più comuni:\n",
      "[('due', 1297), ('senza', 1216), ('fra', 1150), ('quando', 965), ('ogni', 951), ('così', 950), ('me', 914), ('sempre', 871), ('cosa', 838), ('ancora', 801)]\n",
      "Frasi random:\n",
      "['dice', 'mezza', 'voce', 'scatti', 'sangue', 'romagnolo']\n",
      "['madre', 'teneva', 'gambe', 'mostrandolo', 'sotto', 'meraviglia', 'infatti', 'piccolo', 'agitandosi', 'esponeva', 'carni', 'meravigliose', 'freschezza', 'colore']\n",
      "['poteva', 'dire', 'esattezza', 'pena', 'fragili', 'membra', 'sembravano', 'lacerate']\n",
      "['re', 'baldoria']\n",
      "['silenzio']\n"
     ]
    }
   ],
   "source": [
    "checkresults(avanguardie_primo_novecento_senteces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con lemmatizzazione (campione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMMATIZZAZIONE W/ NLTK\n",
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/fasi_letterarie')\n",
    "avanguardie_primo_novecento_senteces_nltk = preprocessing_lem('11_avanguardie_primo_novecento.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing/fasi_letterarie')\n",
    "savesentences('avanguardie_primo_novecento_senteces_nltk', avanguardie_primo_novecento_senteces_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMMATIZZAZIONE W/ SIMPLEMMA\n",
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/fasi_letterarie')\n",
    "avanguardie_primo_novecento_senteces_simplemma = preprocessing_simplemma('11_avanguardie_primo_novecento.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing/fasi_letterarie')\n",
    "savesentences('avanguardie_primo_novecento_senteces_simplemma', avanguardie_primo_novecento_senteces_simplemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMMATIZZAZIONE W/ SPACY \n",
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/fasi_letterarie')\n",
    "avanguardie_primo_novecento_senteces_spacy = preprocessing_spacy('11_avanguardie_primo_novecento.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing/fasi_letterarie')\n",
    "savesentences('avanguardie_primo_novecento_senteces_spacy', avanguardie_primo_novecento_senteces_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/fasi_letterarie')\n",
    "classicismo_arcadia_sentences_spacy = preprocessing_spacy('6_classicismo_arcadia.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing/fasi_letterarie')\n",
    "savesentences('classicismo_arcadia_sentences_spacy', classicismo_arcadia_sentences_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 344934\n",
      "Parole uniche: 50367\n",
      "Parole più comuni:\n",
      "[('due', 1297), ('senza', 1216), ('fra', 1150), ('quando', 965), ('ogni', 951), ('così', 950), ('me', 914), ('sempre', 871), ('cosa', 838), ('ancora', 801)]\n",
      "Frasi random:\n",
      "['dietro', 'minaccia', 'serpenti', 'elefanti']\n",
      "['rendetemela']\n",
      "['bevono', 'dimenticando', 'austriaci']\n",
      "['tre', 'donne', 'avventano', 'signor', 'tenente', 'creda', 'canaglie']\n",
      "['eccoti', 'qua', 'baldoria']\n"
     ]
    }
   ],
   "source": [
    "checkresults(avanguardie_primo_novecento_senteces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 344902\n",
      "Parole uniche: 50262\n",
      "Parole più comuni:\n",
      "[('due', 1297), ('senza', 1216), ('fra', 1150), ('quando', 965), ('ogni', 951), ('così', 950), ('me', 919), ('sempre', 871), ('cosa', 838), ('os', 802)]\n",
      "Frasi random:\n",
      "['giovane', 'brutto', 'forte', 'buona', 'troppo', 'grossolano', 'origine']\n",
      "['funerale']\n",
      "['donna', 'dà', 'milano', 'reticenze', 'mezzetinte', 'parentesi', 'sospensioni', 'spalancherebbe', 'invece', 'brutalmente', 'generosamente', 'nervi', 'spirito', 'corpo', 'stesso', 'uomo', 'trovasse', 'rom']\n",
      "['svalutazione', 'pericolosa', 'aleatoria', 'industria', 'forestiero']\n",
      "['figlio']\n"
     ]
    }
   ],
   "source": [
    "checkresults(avanguardie_primo_novecento_senteces_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 360875\n",
      "Parole uniche: 29090\n",
      "Parole più comuni:\n",
      "[('essere', 12083), ('avere', 6158), ('fare', 2793), ('potere', 1755), ('sapere', 1413), ('volere', 1341), ('vedere', 1319), ('due', 1297), ('dire', 1297), ('cosa', 1267)]\n",
      "Frasi random:\n",
      "['badare', 'prendere', 'responsabilità', 'ricostruire', 'dialogare', 'testualmente']\n",
      "['trentatré', 'anno', 'or', 'essere', 'essere', 'messo', 'lassù', 'camino', 'giusto', 'quando', 'nascere']\n",
      "['sorridere', 'tacendo', 'mentre', 'attraversare', 'persona', 'terrena', 'già', 'trasparire', 'mobile', 'fiorame', 'parete']\n",
      "['silenziare', 'assoluto']\n",
      "['finire', 'casa', 'abitare', 'uomo', 'cominciare', 'popolare', 'scimmia']\n"
     ]
    }
   ],
   "source": [
    "checkresults(avanguardie_primo_novecento_senteces_simplemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 344934\n",
      "Parole uniche: 38502\n",
      "Parole più comuni:\n",
      "[('potere', 1650), ('fare', 1618), ('tutto', 1334), ('due', 1297), ('dire', 1274), ('cosa', 1266), ('vedere', 1259), ('volere', 1246), ('sapere', 1224), ('senza', 1216)]\n",
      "Frasi random:\n",
      "['voce', 'femminile', 'discesa', 'parecchio', 'volta', 'venire', 'vedere', 'cancello', 'bene', 'chiuso', 'comandare', 'eunuchio', 'uccidere ti']\n",
      "['follare', 'infatti', 'disposto', 'assiepatissima', 'tutto', 'cerchio', 'arena']\n",
      "['quando', 'raggiungere', 'grande', 'ritmo', 'umano', 'condurre', 'stupendo', 'città', 'libertà', 'spirituale']\n",
      "['neanche']\n",
      "['due', 'estrema', 'casta', 'dividono', 'suddividono', 'infinito']\n"
     ]
    }
   ],
   "source": [
    "checkresults(avanguardie_primo_novecento_senteces_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams without Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Corpora/fasi_letterarie')\n",
    "avanguardie_primo_novecento_senteces_bigrams = preprocessing_w_bigrams('11_avanguardie_primo_novecento.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parole: 22\n",
      "Parole uniche: 16\n",
      "Parole più comuni:\n",
      "[('n', 3), ('s', 2), ('a', 2), ('o', 2), ('t', 2), ('A', 1), ('l', 1), ('e', 1), ('d', 1), ('r', 1)]\n",
      "Frasi random:\n",
      "o\n",
      "e\n",
      "o\n",
      "r\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/marco/OneDrive - Università degli Studi di Milano-Bicocca/data/Preprocessing/bigrammi/')\n",
    "checkresults('Alessandro Manzoni.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avanguardie_primo_novecento_senteces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marco\\OneDrive\\UNIMIB_DataScience\\99-PROJECTS\\DataSemantics2022\\DataSemantics\\Data-Semantics\\Preprocessing.ipynb Cella 72\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marco/OneDrive/UNIMIB_DataScience/99-PROJECTS/DataSemantics2022/DataSemantics/Data-Semantics/Preprocessing.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m checkresults(avanguardie_primo_novecento_senteces)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avanguardie_primo_novecento_senteces' is not defined"
     ]
    }
   ],
   "source": [
    "checkresults(avanguardie_primo_novecento_senteces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tratta di una cosa che potrebbe avere senso provare a fare mettendo insieme più libri, quindi costruendo i vari corpus in base all'autore, genere letterario o altro. Non ha senso farlo con le frasi di un singolo libro, in quanto, come si vede sotto, le parole più rilevanti risulterebbero quelle usate per costruire frasi di senso compiuto, come verbi o preposizioni. Invece, usando dei corpus che raccolgono più libri, sulla base delle statistiche della parole presenti in ciascun documento rispetto al totale, il modello riesce a individuare l'equilibrio fra i vari argomenti e valutare quali sono i più rilevanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decameron_sentences = open('decameron_frasi.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True rimuove la punteggiatura\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca939f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decameron_words = list(sent_to_words(decameron_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decameron_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Creo un dizionario\n",
    "id2word = corpora.Dictionary(decameron_words)\n",
    "# Creo un corpus\n",
    "texts = decameron_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa46962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Numero di topics\n",
    "num_topics = 10\n",
    "# Definisco il modello LDA\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Stampo le keyword associate ai topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "decameron_sentences[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba18286d27dd3f1705df4025c31d2b7021c67fcd01f023a6f194ed0db21ec453"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
