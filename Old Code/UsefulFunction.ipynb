{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1662973169963,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"h8SntRVn5pG7","outputId":"4bdec4e3-0cdf-4a2f-9438-8cd8d4110850"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["import string\n","import sys\n","import io \n","import nltk\n","import pandas as pd\n","import numpy as np\n","\n","# Stopwords\n","from gensim.parsing.preprocessing import remove_stopwords\n","from nltk.corpus import stopwords \n","nltk.download('stopwords')\n","stops = set(stopwords.words('italian'))\n","nltk_stopwords = nltk.corpus.stopwords.words('italian')\n","# Valutare di aggiungere eventuali forme arcariche delle stopwords italiane, o se c'è necessità di estendere questa lista\n","  \n","# Divido il testo in frasi in base ai punti\n","nltk.download('punkt')\n","\n","# Tokenizer\n","from nltk.tokenize import word_tokenize \n","\n","# Per il discorso lemmatizzazione dobbiamo valutare come muoverci con l'italiano\n","\n","# Lemmatization\n","from nltk.stem import WordNetLemmatizer \n","from nltk.corpus import wordnet\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","# Contatore parole uniche\n","from collections import Counter\n","\n","# Per esplorare risultati\n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5391,"status":"ok","timestamp":1662972932584,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"_y9jpJy16A3G","outputId":"0f4648cf-2590-46c8-fec8-1d9e2a8e6b13"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cade in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cade) (1.21.6)\n","Requirement already satisfied: smart-open==1.10.0 in /usr/local/lib/python3.7/dist-packages (from cade) (1.10.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from cade) (3.2.2)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from cade) (0.29.32)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from smart-open==1.10.0->cade) (1.24.70)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from smart-open==1.10.0->cade) (2.23.0)\n","Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from smart-open==1.10.0->cade) (1.18.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->smart-open==1.10.0->cade) (0.6.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->smart-open==1.10.0->cade) (1.0.1)\n","Requirement already satisfied: botocore<1.28.0,>=1.27.70 in /usr/local/lib/python3.7/dist-packages (from boto3->smart-open==1.10.0->cade) (1.27.70)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.70->boto3->smart-open==1.10.0->cade) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.70->boto3->smart-open==1.10.0->cade) (1.25.11)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.70->boto3->smart-open==1.10.0->cade) (1.15.0)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->smart-open==1.10.0->cade) (1.0.3)\n","Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->smart-open==1.10.0->cade) (0.4.1)\n","Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->smart-open==1.10.0->cade) (1.35.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open==1.10.0->cade) (57.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open==1.10.0->cade) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open==1.10.0->cade) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open==1.10.0->cade) (4.2.4)\n","Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open==1.10.0->cade) (1.31.6)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open==1.10.0->cade) (2022.2.1)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open==1.10.0->cade) (21.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open==1.10.0->cade) (1.56.4)\n","Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open==1.10.0->cade) (3.17.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open==1.10.0->cade) (3.0.9)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage->smart-open==1.10.0->cade) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->smart-open==1.10.0->cade) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->smart-open==1.10.0->cade) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->smart-open==1.10.0->cade) (3.0.4)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cade) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cade) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->cade) (4.1.1)\n"]}],"source":["!pip install cade"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1662972932584,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"P38AxNDj51YK"},"outputs":[],"source":["import os\n","from gensim.models import Word2Vec\n","from cade.cade import CADE"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18353,"status":"ok","timestamp":1662972950932,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"t5RTjad76JCl","outputId":"f83e7a7d-0da2-4dc3-93a7-df484a07100d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"K5CTFRCF6YW6"},"source":["### CREO TUTTO IL NECESSARIO PER TESTARE LE FUNZIONI"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1662972998992,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"ViMPOpF06ers"},"outputs":[],"source":["# Preprocessing (NO lemmatization)\n","def preprocessing(file_name):\n","\n","  output=\"\"\n","  with open(file_name, encoding='utf-8') as f:\n","      for line in f:\n","          if not line.isspace(): # Rimuovo linee vuote\n","              output+=line\n","\n","  # Divido il testo in frasi, basandomi sui punti\n","  output_sentences = nltk.tokenize.sent_tokenize(output)\n","\n","  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n","  # output_sentences = output_sentences[:-]\n","\n","  filtered_sentences = []\n","  # 'Pulisco' ogni frasi, una alla volta\n","  for sentence in output_sentences:\n","    # Metto tutto in lower case\n","    lower_sentence=sentence.lower()\n","    # Rimuovo caratteri non alfa numerici\n","    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n","    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n","    filtered_sentence = [w for w in noalfa_sentence if ((w not in stops) and (len(w) > 1))]\n","    # Ricostruisco la lista con le frasi 'pulite'\n","    if filtered_sentence:\n","      filtered_sentences.append(filtered_sentence)\n","\n","  return filtered_sentences"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":706,"status":"ok","timestamp":1662973116795,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"DYHMKQtP6jr-"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/DataSemanticsProject/Books')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8491,"status":"ok","timestamp":1662973126502,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"sqlDUZg36930"},"outputs":[],"source":["frasi_decameron = preprocessing('Decameron.txt')\n","frasi_orlando = preprocessing('Orlando furioso.txt')"]},{"cell_type":"markdown","metadata":{"id":"QhDEXjRc7E0X"},"source":["### DEFINIZIONE FUNZIONI"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":220,"status":"ok","timestamp":1662974195065,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"SNvludfbZDZM"},"outputs":[],"source":["# Funzione per addestrare n modelli per corpus\n","\n","def training_W2V(sentences, text, n_mod): #file con frasi del corpus, nome del corpus, numero di modelli da addestrare\n","\n","  for k in range(n_mod):\n","    model = Word2Vec(sentences = sentences,\n","                    #window = 5, default value\n","                    min_count=10, #not consider word with absolute frequency <10 \n","                    size=300, #vector size \n","                    sg = 1, #skipgram algorithm\n","                    hs = 0,\n","                    negative = 5, #negative sampling with 5 noise words\n","                    workers = 5, #faster process\n","                    iter = 6 #6 iterations\n","                    )\n","  \n","    model.save(text.lower() + \"_\" + str(k) + \".model\")\n","\n","\n","# Funzione per addestrare n slice per corpus con CADE\n","\n","# !cat corpus1.txt corpus2.txt corpus3.txt ... > compass.txt\n","\n","def training_CADE(texts, n_mod): #lista contenente i corpus usati per creare la compass in ordine, numero di slices da addestrare\n","                                 #la lista dev'essere composta dal nome esatto del file txt ma senza l'estensione\n","\n","  aligner = CADE(min_count=10,  \n","                  size=300,\n","                  sg = 1, \n","                  #hs = 0,\n","                  ns = 5, \n","                  workers = 5,\n","                  siter = 6)\n","\n","  for k in range(n_mod):\n","    aligner.train_compass('compass.txt', overwrite=True)\n","    for text in texts:\n","      model_slice = aligner.train_slice(text + \".txt\") #per trainare le slice ho bisogno del nome esatto del file .txt con cui ho creato il compasso\n","      model_slice.save(text + '_cade_' + str(k) + '.model') #qui posso salvare il modello con un nome a piacere"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":787,"status":"ok","timestamp":1662974471981,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"VCCMJxvjX1jN"},"outputs":[],"source":["# Funzione per estrarre le n parole più simili ad una parola target, calcolando la similarità media, per un dato numero di modelli creati\n","# Ipotizzo in questo caso di aver creato un numero n_mod di modelli, così da rendere più stabili i risultati\n","\n","def similar_words_per_topos(word, n_mod, n_words, text): #parola target da analizzare, numero di modelli da considerare, numero di parole simili da estrarre, corpus da analizzare\n","\n","  #os.chdir('') #inserire path alla cartella con i corpus\n","  text_2 = text\n","  text = text.lower()\n","\n","  #ottengo gli n modelli del testo considerato\n","  embeddings = []\n","  for k in range(n_mod):\n","    model = Word2Vec.load(text.lower() + '_' + str(k) + '.model') #adattare eventualmente nome e path dei modelli per il caricamento\n","    embeddings.append(model) #ottengo una lista contenente gli n modelli, precedentemente creati, per il testo desiderato\n","\n","  #modello per modello, ottengo la lista delle m parole (n_words) più simili al target\n","  words = []\n","  for k in range(n_mod):\n","    for tupla in embeddings[k].wv.most_similar(word)[:n_words]: #l'output di most_similar è una lista di tuple che presenta la parola e il rispettivo valore di similarità\n","      words.append(tupla[0]) #appendo solamente la parola, senza considerare il valore di similarità rispetto al target\n","\n","  #creo un dizionario che presenta le parole uniche contenute nella lista \"words\", e le rispettive frequenze assolute\n","  c = Counter(words)\n","  word_frequencies = dict(c)\n","\n","  #creo un dizionario che contiene la lista di parole uniche in \"words\" e calcola la similarità media di ciascuna di esse rispetto alla parola target\n","  init = [0] * len(np.unique(words))\n","  word_similarities = dict(zip(np.unique(words), init))\n","\n","  for k in range(n_mod):\n","    for tupla in embeddings[k].wv.most_similar(word)[:n_words]:\n","      word_similarities[tupla[0]] += tupla[1]\n","  for word in np.unique(words):\n","    word_similarities[word] = word_similarities[word] / word_frequencies[word]\n","\n","  #trasformo il dizionario con le frequenze assolute e quello con le similarità medie in dataframe, e li unisco\n","  df_1 = pd.DataFrame(list(word_frequencies.items()), columns=['Words','Abs.frequency'])\n","  df_2 = pd.DataFrame(list(word_similarities.items()), columns=['Words','Med.similarity'])\n","  #calcolo anche la similarità normalizzata, nel caso dovesse servie\n","  normalized_similarity = (df_2['Med.similarity']-df_2['Med.similarity'].min())/(df_2['Med.similarity'].max() - df_2['Med.similarity'].min()) \n","  df = pd.merge(df_1, df_2, on = 'Words')\n","  df['Med.similarity(norm.)'] = normalized_similarity\n","\n","  #aggiungo la colonna col nome del testo considerato\n","  df['Text'] = [text_2] * len(df)\n","\n","  return df"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1662975240509,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"yX8O88P1tPcO"},"outputs":[],"source":["# Funzione per comparare una parola fra embedding diversi\n","\n","def compare_word_btw_embeddings(word_to_study, n_mod, text1, text2): #parola da studiare, numero di embedding creati e da studiare, testo 1 preso come riferimento (da questo testo\n","                                                                     # estraggo il vettore della parola target), testo 2 da cui estraggo le parole simili\n","\n","  #prendo il numero totale di embedding dei due testi considerati\n","  embeddings1 = []\n","  for it in range(n_mod):\n","    model = Word2Vec.load(text1 + '_cade_' + str(it) + '.model') #ho scritto cade perchè tendenzialmente questa funzione sarà usata su embedding allineati\n","    embeddings1.append(model)\n","\n","  embeddings2 = []\n","  for it in range(n_mod):\n","    model = Word2Vec.load(text2 + '_cade_' + str(it) + '.model')\n","    embeddings2.append(model)\n","\n","  #ottengo la lista delle parole più simili alla target per ciascun modello\n","  words = []\n","  for k in range(n_mod):\n","    word_vector = embeddings1[k].wv[word_to_study]\n","    for tupla in embeddings2[k].wv.most_similar(positive=[word_vector])[:10]: #ho preso arbitrariamente 10 parole, si può modificare a piacimento\n","      words.append(tupla[0])\n","\n","  #creo dizionario con parole uniche e relativa frequenza assoluta\n","  c = Counter(words)\n","  word_frequencies = dict(c)\n","\n","  #creo dizionario con parole uniche calcolando la similarità media\n","  init = [0] * len(np.unique(words))\n","  word_similarities = dict(zip(np.unique(words), init))\n","\n","  for k in range(n_mod):\n","    word_vector = embeddings1[k].wv[word_to_study]\n","    for tupla in embeddings2[k].wv.most_similar(positive=[word_vector])[:10]:\n","      word_similarities[tupla[0]] += tupla[1]\n","  for word in np.unique(words):\n","    word_similarities[word] = word_similarities[word] / word_frequencies[word]\n","  \n","  #genero il dataframe finale\n","  df = pd.DataFrame(list(word_similarities.items()), columns=['Words','Med.similarity'])\n","  df = df.sort_values(by=['Med.similarity'], ascending=False)\n","  df = df.head(n=10)\n","  return df\n","\n","\n","\n","  ### DA USARE POI NEL SEGUENTE CICLO\n","  #for text in corpora:\n","  #  print('***' + text +'***')\n","  #  df = compare_word_btw_embeddings('parola', 'testo di riferimento', text)\n","  #  print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgaFWAETbcyQ"},"outputs":[],"source":["# Funzione per creare una heatmap di frequenza delle parole più simili a una parola target rispetto a vari corpus (presa da Simo, legata alla funzione precedente)\n","\n","def freq_heatmap_preparation(column, word, n_mod, n_words): #colonna da considerare nella heatmap (frequenza assoluta o similarità media), parola target, numero di modelli da \n","                                                            #considerare, numero di parole da considerare\n","  dfs_list = []\n","  corpus_texts = ['SCRIVERE'] #inserire i nomi dei corpus su cui iterare la funzione (VANNO INSERITI MANUALMENTE)\n","\n","  for text in corpus_texts:\n","    dfs_list.append(explore_word(word, n_mod, n_words, text)) #richiamo la funzione precedente e appendo i dataframe creati a una lista di dataframes\n","\n","  result = pd.concat(dfs_list, ignore_index=True) #concateno i dataframes appesi alla lista\n","  df = result[['Words', column, 'Text']] #riordino a piacimento le colonne\n","  df_hm = df.pivot_table(index='Text', columns='Words', values=column) #effettuo un pivot per avere i dati pronti da inserire nella heatmap\n","\n","  return df_hm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrUgYaqTdEKv"},"outputs":[],"source":["# Funzione per generare la heatmap con i dati preparati con la funzione precedente\n","\n","def plot_the_heatmap(df_heatmap, word, column): #dataframe preparato con funzione precedente, parola target, colonna analizzata (frequenza assoluta o similarità media)\n","  #imposto la heatmap (cambiare a piacimento)\n","  plt.figure(figsize=(20,4))\n","  cmap = sns.color_palette(\"Greens\", as_cmap=True) \n","\n","  if(column == 'Abs.frequency'):\n","    bounds = [] #inserire manualmente i bounds della heatmap a seconda dei nostri valori di frequenza\n","    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n","    hm = sns.heatmap(df_heatmap, cmap=cmap, norm=norm, cbar_kws={\"shrink\": 1.2}, linewidths=1)\n","\n","  elif((column == 'Med.similarity') or (column == 'Med.similarity(norm.)')):\n","    hm = sns.heatmap(df_heatmap, cmap=cmap, cbar_kws={\"shrink\": 1.2}, linewidths=1)\n","\n","  else:\n","    print('Colonna sbagliata')\n","    return\n","\n","  hm.set_xlabel(xlabel = 'Words', fontsize = 16)\n","  hm.set_ylabel(ylabel = column , fontsize = 17)\n","  hm.set_xticklabels(hm.get_xmajorticklabels(), fontsize = 14)\n","  hm.set_yticklabels(hm.get_ymajorticklabels(), fontsize = 14)\n","  hm.axes.set_title(column + \" INSERIRE TITOLO \"+ word, fontsize=20)\n","  hm.set_facecolor('lightgrey')\n","  plt.savefig('NOME_FIGURA')"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":257,"status":"ok","timestamp":1662975929261,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"ufIYZ-yza8CQ"},"outputs":[],"source":["# Funzione per esplorare analogie e calcolare similarità media dati tutti i modelli addestrati per un corpus\n","# SE NECESSARIO, USARE LA STESSA FUNZIONE ANCHE PER I MODELLI ALLINEATI ADDESTRATI CON CADE, VA SOLO CAMBIATO IL NOME DEL FILE NEL PRIMO for\n","\n","def compute_analogies(text, n_mod, word1, word2, word3): #corpus su cui eseguire le analogie, numero di modelli addestrati, word1 & word2 positive, word3 negativa\n","  models = []\n","\n","  for k in range(n_mod): #inserire manualmente il numero di modelli addestrati \n","    model = Word2Vec.load(text+'_'+str(k)+'.model') #sistemare il nome in base a come sono nominati i modelli\n","    models.append(model) #appendo i modelli ad una lista\n","\n","  list_tuples = []\n","  list_words = []\n","\n","  for i in range(n_mod):\n","    list_tuples.append(models[i].most_similar(positive=[word1,word2], negative=[word3])) #per tutti i modelli caricati calcolo l'analogia (ho delle tuple con (parola, similarità))\n","      \n","  for i in list_tuples:\n","    for element in i:\n","      list_words.append(element) #appendo le tuple una per una \n","\n","  \n","\n","  df = pd.DataFrame(list_words, columns=['Words', 'Cosine_similarity']) #creo un dataframe\n","  df = df.groupby('Words').mean() #raggruppo per parola calcolando la media\n","  df.sort_values(by=['Cosine_similarity'], inplace = True, ascending = False) #ordino per similarità massima\n","  return df       "]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":250,"status":"ok","timestamp":1662976313819,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"mLGK_-ur09hg"},"outputs":[],"source":["# Funzioni per estrarre sinonimi di una certa parola target, e per estrarre GLI AGGETTIVI più simili a quella parola (o insieme di parole, considerando i sinonimi)\n","# LE SEGUENTI FUNZIONI POSSONO ESSERE USATE SIA PER I MODELLI DI WORD2VEC CHE PER QUELLI ALLINEATI DI CADE\n","\n","#models = [] #per caricare i modelli pre-addestrati\n","#for k in range(n_mod):\n","  #model = Word2Vec.load(text+'_'+str(k)+'.model')\n","  #models.append(model)\n","\n","\n","def find_similar(word: str, n_similar: int, n_mod:int, pos_tag_prefix: str, models: list) -> list: #parola da analizzare, numero di parole simili volute, numero modelli, prefisso POS \n","                                                                                                   #desiderato, lista dei modelli\n","\n","  most_similar = dict()\n","\n","  for k in range(n_mod):\n","    #ottengo la lista delle n parole simili alla parola target per modello, e filtro mantenendo solo quelle che hanno il POS tag desiderato\n","    similar = models[k].wv.most_similar(word, topn=(5*n_similar)) #moltiplico il numero di parole per avere un campione più ampio\n","    similar_filtered = list(filter(lambda similar: check_pos_tag(similar[0], pos_tag_prefix), similar))\n","\n","    #combino le parole ottenute dai vari modelli\n","    for (string, similarity) in similar_filtered:\n","      if string not in most_similar.keys():\n","        most_similar[string] = similarity\n","      else:\n","        most_similar[string] = max(most_similar[string], similarity)\n","\n","  #ordino le parole in base alla similarità e seleziono solo il numero desiderato\n","  similar_filtered_sorted = sorted(most_similar.items(), key=lambda item: item[1], reverse=True)[:n_similar]\n","  similar_final = [w for (w, sim) in similar_filtered_sorted]\n","\n","  return similar_final\n","\n","\n","\n","\n","\n","# Funzione utile in quella precedente, che definisce il \"part-of-speech\" tag\n","# tagger = treetaggerwrapper.TreeTagger(TAGLANG='it')\n","\n","def check_pos_tag(word: str, tag_prefix: str) -> bool:\n","  tag = tagger.tag_text(word)\n","  part_of_speech = treetaggerwrapper.make_tags(tag)\n","  return part_of_speech[0].pos.startswith(tag_prefix)\n","\n","\n","# Declinazione della funzione generale per trovare i sinonimi\n","\n","def find_synonyms(word: str, n_synonyms: int, n_mod: int, models: list) -> list:\n","  return find_similar(word, n_synonyms, n_mod, 'NOM', models) #sistemare, se necessario, il tag\n","\n","# Declinazione della funzione generale per trovare gli aggettivi descrittivi\n","\n","def find_describing_adjectives(words: list, n_adjectives: int, n_mod: int, models: list) -> None: #in questo caso passo la lista contenente i sinonimi\n","  for word in words:\n","    print(find_similar(word, n_adjectives, n_mod, 'ADJ', models))"]},{"cell_type":"markdown","metadata":{"id":"ErSZHXwb7ehF"},"source":["### TEST FUNZIONI"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53253,"status":"ok","timestamp":1662973568295,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"CpPEb9SH7hbB","outputId":"a2e006b0-0fa9-4a0d-a3f6-65a10a43b165"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"]}],"source":["# Test funzione di training WORD2VEC\n","\n","training_W2V(frasi_decameron, \"decameron\", 5)\n","training_W2V(frasi_orlando, \"orlandofurioso\", 5)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163194,"status":"ok","timestamp":1662974363963,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"Rm5jPpPF8lUn","outputId":"fed82503-4210-463b-e1e6-f690c7bd6dd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training the compass from scratch.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"]},{"name":"stdout","output_type":"stream","text":["Training embeddings: slice Decameron.txt.\n","Initializing embeddings from compass.\n","Training embeddings: slice Orlando furioso.txt.\n","Initializing embeddings from compass.\n","Training the compass from scratch.\n","Initializing embeddings from compass.\n","Training embeddings: slice Decameron.txt.\n","Initializing embeddings from compass.\n","Training embeddings: slice Orlando furioso.txt.\n","Initializing embeddings from compass.\n","Training the compass from scratch.\n","Initializing embeddings from compass.\n","Training embeddings: slice Decameron.txt.\n","Initializing embeddings from compass.\n","Training embeddings: slice Orlando furioso.txt.\n","Initializing embeddings from compass.\n","Training the compass from scratch.\n","Initializing embeddings from compass.\n","Training embeddings: slice Decameron.txt.\n","Initializing embeddings from compass.\n","Training embeddings: slice Orlando furioso.txt.\n","Initializing embeddings from compass.\n","Training the compass from scratch.\n","Initializing embeddings from compass.\n","Training embeddings: slice Decameron.txt.\n","Initializing embeddings from compass.\n","Training embeddings: slice Orlando furioso.txt.\n","Initializing embeddings from compass.\n"]}],"source":["# Test funzione di training CADE\n","\n","!cat Decameron.txt Orlando\\ furioso.txt > compass.txt\n","lista_testi = ['Decameron', 'Orlando furioso']\n","\n","training_CADE(lista_testi, 5)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1549,"status":"ok","timestamp":1662974765606,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"qNsJa1S2AN2j","outputId":"24b97750-be70-4874-cd7d-a6092b0831a9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"]},{"name":"stdout","output_type":"stream","text":["         Words  Abs.frequency  Med.similarity  Med.similarity(norm.)  \\\n","0       gentil              5        0.975001               0.134948   \n","1    figliuola              5        0.955136               0.139902   \n","2        forte              5        0.951564               0.137129   \n","3    ricciardo              5        0.945880               0.008638   \n","4      rinaldo              4        0.927420               0.655332   \n","5       moglie              5        0.934323               0.593367   \n","6        padre              1        0.929289               1.000000   \n","7        buona              1        0.925428               0.302578   \n","8       pietro              2        0.929491               0.105911   \n","9   maravigliò              3        0.920242               0.103417   \n","10     giovane              3        0.923469               0.049917   \n","11   cavaliere              2        0.925268               0.027037   \n","12      udendo              1        0.924786               0.294233   \n","13        viso              1        0.924102               0.206897   \n","14       bella              1        0.925143               0.210390   \n","15       fante              1        0.917863               0.494745   \n","16     vedendo              2        0.917365               0.174461   \n","17      gianni              1        0.934804               0.128761   \n","18      gliele              1        0.923325               0.000000   \n","19      marito              1        0.918923               0.116890   \n","\n","         Text  \n","0   Decameron  \n","1   Decameron  \n","2   Decameron  \n","3   Decameron  \n","4   Decameron  \n","5   Decameron  \n","6   Decameron  \n","7   Decameron  \n","8   Decameron  \n","9   Decameron  \n","10  Decameron  \n","11  Decameron  \n","12  Decameron  \n","13  Decameron  \n","14  Decameron  \n","15  Decameron  \n","16  Decameron  \n","17  Decameron  \n","18  Decameron  \n","19  Decameron  \n","         Words  Abs.frequency  Med.similarity  Med.similarity(norm.)  \\\n","0        bella              5        0.976432               0.009613   \n","1      cortese              5        0.962067               0.230155   \n","2   bradamante              5        0.964810               1.000000   \n","3           sì              2        0.960893               0.487885   \n","4       parlar              3        0.959930               0.367003   \n","5         nome              3        0.956875               0.086201   \n","6         fede              4        0.957754               0.120406   \n","7       costei              3        0.955694               0.003089   \n","8       moglie              3        0.955640               0.176974   \n","9        potea              2        0.956314               0.114156   \n","10       parea              1        0.957331               0.270693   \n","11       forte              2        0.956329               0.410792   \n","12       avria              2        0.958961               0.083800   \n","13        pare              1        0.953738               0.138241   \n","14         men              1        0.959881               0.000000   \n","15        amor              1        0.953956               0.158340   \n","16       dicea              1        0.953808               0.272845   \n","17        così              2        0.956471               0.113513   \n","18     ruggier              1        0.970253               0.727735   \n","19       valor              1        0.963228               0.315276   \n","20       mente              1        0.963060               0.376845   \n","21       torto              1        0.962290               0.418162   \n","\n","              Text  \n","0   Orlandofurioso  \n","1   Orlandofurioso  \n","2   Orlandofurioso  \n","3   Orlandofurioso  \n","4   Orlandofurioso  \n","5   Orlandofurioso  \n","6   Orlandofurioso  \n","7   Orlandofurioso  \n","8   Orlandofurioso  \n","9   Orlandofurioso  \n","10  Orlandofurioso  \n","11  Orlandofurioso  \n","12  Orlandofurioso  \n","13  Orlandofurioso  \n","14  Orlandofurioso  \n","15  Orlandofurioso  \n","16  Orlandofurioso  \n","17  Orlandofurioso  \n","18  Orlandofurioso  \n","19  Orlandofurioso  \n","20  Orlandofurioso  \n","21  Orlandofurioso  \n"]}],"source":["# Test funzione parole simili per topos\n","\n","df = similar_words_per_topos(\"donna\", 5, 10, \"Decameron\")\n","df2 = similar_words_per_topos(\"donna\", 5, 10, \"Orlandofurioso\")\n","print(df)\n","print(df2)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3072,"status":"ok","timestamp":1662975336664,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"MqwPszwSB2hg","outputId":"7e99dbc6-cc50-4a53-a761-eb3f528e1329"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"]},{"name":"stdout","output_type":"stream","text":["        Words  Med.similarity\n","7       donna        0.801823\n","17     regina        0.634103\n","19    vecchia        0.631947\n","18    sorella        0.568712\n","0   Discordia        0.508708\n","1    Doralice        0.500033\n","9        fata        0.498671\n","6    difender        0.497261\n","11    giovane        0.492326\n","14       maga        0.487076\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"]},{"name":"stdout","output_type":"stream","text":["         Words  Med.similarity\n","5        donna        0.801823\n","15  giovinetta        0.653398\n","1      Bernabò        0.643578\n","13       giace        0.635454\n","10      fante,        0.581835\n","4      domanda        0.543919\n","19       monna        0.543856\n","11  fanticella        0.536388\n","14     giovane        0.530860\n","3    cameriera        0.530698\n"]}],"source":["# Test funzione per comparazione parole fra embeddings\n","# In questo caso prendo come riferimento il Decameron, da cui estraggo il vettore della parola target \"donna\", e confronto con i vettori dell'altro testo\n","\n","df3 = compare_word_btw_embeddings(\"donna\", 5, \"Decameron\", \"Orlando furioso\")\n","print(df3)\n","\n","# Ora provo il contrario\n","\n","df4 = compare_word_btw_embeddings(\"donna\", 5, \"Orlando furioso\", \"Decameron\")\n","print(df4)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":975,"status":"ok","timestamp":1662975951622,"user":{"displayName":"Gianluca Cavallaro","userId":"01957991007423415276"},"user_tz":-120},"id":"fbMae8p5EsDG","outputId":"eca8648f-fcdb-4e6d-a68e-00fe79adc0e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"]},{"name":"stdout","output_type":"stream","text":["DECAMERON: uomo-amore+guerra\n","            Cosine_similarity\n","Words                        \n","chiamato             0.935694\n","nome                 0.904417\n","gentile              0.888838\n","ricco                0.884999\n","messer               0.880452\n","torello              0.837737\n","buono                0.831476\n","mercatante           0.827906\n","adunque              0.820707\n","figliuolo            0.819537\n","chiamata             0.817307\n","guiglielmo           0.806817\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  from ipykernel import kernelapp as app\n"]}],"source":["# Test funzione analogie\n","\n","df5 = compute_analogies(\"decameron\", 5, \"uomo\", \"guerra\", \"amore\")\n","print('DECAMERON: uomo-amore+guerra')\n","print(df5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Funzione che mergia due file di testo sfruttando la secure hashes. La tengo qua in caso serva (ho usato la procedura che aveva aggiunto Gian\n","# in quanto andava ad aggiungere le stopwords direttamente al file della libreria NTLK, oltre che risultare molto più semplice)\n","\n","# Procedura che combina n files in un unico file non considerando i duplicati\n","\n","def get_sha1(file):\n","    checksum = hashlib.sha1()\n","    for chunk in iter(lambda: file.read(4096), b\"\"):\n","        checksum.update(chunk)\n","    return checksum.hexdigest()\n","\n","def already_copied(file, checksums):\n","    checksum = get_sha1(file)\n","    if checksum not in checksums:\n","        checksums.add(checksum)\n","        return False\n","    return True\n","\n","checksums = set()\n","with open(\"Output/stopwords_merged.txt\", \"wb\") as merged:\n","    for file in glob.glob(\"Output/stop*.txt\"):\n","        with open(file, \"rb\") as file:\n","            if already_copied(file, checksums):\n","                continue\n","            file.seek(0) # Ritorno all'inizio del file\n","            for line in file:\n","                merged.write(line)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM3c1A2A7UCt9ooRye+oMlB","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.3 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.3"},"vscode":{"interpreter":{"hash":"ba18286d27dd3f1705df4025c31d2b7021c67fcd01f023a6f194ed0db21ec453"}}},"nbformat":4,"nbformat_minor":0}
