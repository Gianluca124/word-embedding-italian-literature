{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e638272a",
   "metadata": {},
   "source": [
    "# Importazione librerie e funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15003dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cafe6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cafe6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cafe6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cafe6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\cafe6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import sys\n",
    "import io \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('italian'))\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('italian')\n",
    "# Valutare di aggiungere eventuali forme arcariche delle stopwords italiane, o se c'è necessità di estendere questa lista\n",
    "  \n",
    "# Divido il testo in frasi in base ai punti\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Per il discorso lemmatizzazione dobbiamo valutare come muoverci con l'italiano\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Contatore parole uniche\n",
    "from collections import Counter\n",
    "\n",
    "# Per esplorare risultati\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51176db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (NO lemmatization)\n",
    "def preprocessing(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frasi, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    filtered_sentence = [w for w in noalfa_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if filtered_sentence:\n",
    "      filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc70155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from cade.cade import CADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfaf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per addestrare n modelli per corpus\n",
    "\n",
    "def training_W2V(sentences, text, n_mod): #file con frasi del corpus, nome del corpus, numero di modelli da addestrare\n",
    "\n",
    "  for k in range(n_mod):\n",
    "    model = Word2Vec(sentences = sentences,\n",
    "                    #window = 5, default value\n",
    "                    min_count=10, #not consider word with absolute frequency <10 \n",
    "                    size=300, #vector size \n",
    "                    sg = 1, #skipgram algorithm\n",
    "                    hs = 0,\n",
    "                    negative = 5, #negative sampling with 5 noise words\n",
    "                    workers = 5, #faster process\n",
    "                    iter = 6 #6 iterations\n",
    "                    )\n",
    "  \n",
    "    model.save(text.lower() + \"_\" + str(k) + \".model\")\n",
    "\n",
    "\n",
    "# Funzione per addestrare n slice per corpus con CADE\n",
    "\n",
    "# !cat corpus1.txt corpus2.txt corpus3.txt ... > compass.txt\n",
    "\n",
    "def training_CADE(texts, n_mod): #lista contenente i corpus usati per creare la compass in ordine, numero di slices da addestrare\n",
    "                                 #la lista dev'essere composta dal nome esatto del file txt ma senza l'estensione\n",
    "\n",
    "  aligner = CADE(min_count=10,  \n",
    "                  size=300,\n",
    "                  sg = 1, \n",
    "                  #hs = 0,\n",
    "                  ns = 5, \n",
    "                  workers = 5,\n",
    "                  siter = 6)\n",
    "\n",
    "  for k in range(n_mod):\n",
    "    aligner.train_compass('compass.txt', overwrite=True)\n",
    "    for text in texts:\n",
    "      model_slice = aligner.train_slice(text + \".txt\") #per trainare le slice ho bisogno del nome esatto del file .txt con cui ho creato il compasso\n",
    "      model_slice.save(text + '_cade_' + str(k) + '.model') #qui posso salvare il modello con un nome a piacere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a0b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def simple_preproc(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3cb7c",
   "metadata": {},
   "source": [
    "# Prova training w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ea98839",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/cafe6/Università degli Studi di Milano-Bicocca/g.carbone8@campus.unimib.it - data')\n",
    "path_periodi_letterari = 'C:/Users/cafe6/Università degli Studi di Milano-Bicocca/g.carbone8@campus.unimib.it - data/Preprocessing/fasi_letterarie/Frasi NON lemmatizzate/'\n",
    "path_periodi_storici = 'C:/Users/cafe6/Università degli Studi di Milano-Bicocca/g.carbone8@campus.unimib.it - data/Preprocessing/periodi_storici/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84299b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_periodi_letterari + '7_illuminismo_neoclassicismo.txt', encoding='utf-8') as read_file:\n",
    "    illuminismo_neoclassicismo_sentences = [simple_preproc(k).lower().split() for k in read_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56970182",
   "metadata": {},
   "outputs": [],
   "source": [
    "illuminismo_neoclassicismo_model = Word2Vec(sentences = illuminismo_neoclassicismo_sentences,\n",
    "                                            #window = 5, default value\n",
    "                                            min_count=10, #not consider word with absolute frequency <10\n",
    "                                            vector_size=300, #vector size\n",
    "                                            sg = 1, #skipgram algorithm\n",
    "                                            hs = 0,\n",
    "                                            negative = 5, #negative sampling with 5 noise words\n",
    "                                            workers = 5, #faster process\n",
    "                                            epochs = 6 #6 iterations\n",
    "                                           )\n",
    "\n",
    "# circa 1 minuto per il training (file 14MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2692321c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sposa', 0.7747417688369751),\n",
       " ('madre', 0.7651275396347046),\n",
       " ('amata', 0.7642540335655212),\n",
       " ('unica', 0.7609820365905762),\n",
       " ('sposo', 0.7592434287071228),\n",
       " ('ottima', 0.7581976652145386),\n",
       " ('egregia', 0.7551229596138),\n",
       " ('virtuosa', 0.7550404667854309),\n",
       " ('accorta', 0.7507830262184143),\n",
       " ('avvenente', 0.7493323087692261)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illuminismo_neoclassicismo_model.wv.most_similar('donna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28c7e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_periodi_letterari + '8_romanticismo.txt', encoding='utf-8') as read_file:\n",
    "    romanticismo_sentences = [simple_preproc(k).lower().split() for k in read_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2374ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "romanticismo_model = Word2Vec(sentences = romanticismo_sentences,\n",
    "                                            #window = 5, default value\n",
    "                                            min_count=10, #not consider word with absolute frequency <10\n",
    "                                            vector_size=300, #vector size\n",
    "                                            sg = 1, #skipgram algorithm\n",
    "                                            hs = 0,\n",
    "                                            negative = 5, #negative sampling with 5 noise words\n",
    "                                            workers = 5, #faster process\n",
    "                                            epochs = 6 #6 iterations\n",
    "                                           )\n",
    "\n",
    "# circa 3 minuti per il training (file 45MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8dab77ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dama', 0.5912120938301086),\n",
       " ('prassede', 0.5745195150375366),\n",
       " ('ragazza', 0.5625020861625671),\n",
       " ('costei', 0.5556043982505798),\n",
       " ('fanciulla', 0.5509609580039978),\n",
       " ('marito', 0.5431538224220276),\n",
       " ('femmina', 0.5390230417251587),\n",
       " ('vedova', 0.5338456034660339),\n",
       " ('pudica', 0.532570481300354),\n",
       " ('orfana', 0.530032753944397)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romanticismo_model.wv.most_similar('donna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43dcad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_periodi_storici + '1814_1860_Risorgimento.txt', encoding='utf-8') as read_file:\n",
    "    risorgimento_sentences = [simple_preproc(k).lower().split() for k in read_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ecc1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "risorgimento_model = Word2Vec(sentences = risorgimento_sentences,\n",
    "                                            #window = 5, default value\n",
    "                                            min_count=10, #not consider word with absolute frequency <10\n",
    "                                            vector_size=300, #vector size\n",
    "                                            sg = 1, #skipgram algorithm\n",
    "                                            hs = 0,\n",
    "                                            negative = 5, #negative sampling with 5 noise words\n",
    "                                            workers = 5, #faster process\n",
    "                                            epochs = 6 #6 iterations\n",
    "                                           )\n",
    "\n",
    "# circa 5 minuti per il training (file 83MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e98b8e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paola', 0.5707671642303467),\n",
       " ('fanciulla', 0.5651019811630249),\n",
       " ('donzella', 0.5281831622123718),\n",
       " ('gentildonna', 0.5272212624549866),\n",
       " ('clelia', 0.5271718502044678),\n",
       " ('colei', 0.5255793333053589),\n",
       " ('femmina', 0.5114826560020447),\n",
       " ('costei', 0.507268488407135),\n",
       " ('piccarda', 0.4998212158679962),\n",
       " ('prassede', 0.4954227805137634)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risorgimento_model.wv.most_similar('donna')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8322b2",
   "metadata": {},
   "source": [
    "# Ricostruzione corpus per CADE e prova training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f79c79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10_decadentismo.txt', '11_avanguardie_primo_novecento.txt', '12_neorealismo.txt', '1_duecento_poetica_e_prosa Medievo.txt', '2_dolce_stil_novo.txt', '3_petrarchismo.txt', '4_boccaccio_e_umanesimo.txt', '5_manierismo_e_barocco.txt', '6_classicismo_arcadia.txt', '7_illuminismo_neoclassicismo.txt', '8_romanticismo.txt', '9_verismo.txt']\n"
     ]
    }
   ],
   "source": [
    "files = [f for f in os.listdir(path_periodi_letterari)]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f0720e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cade_periodi_letterari = 'C:/Users/cafe6/Università degli Studi di Milano-Bicocca/g.carbone8@campus.unimib.it - data/Preprocessing/fasi_letterarie/Corpus CADE/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8efee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(path_periodi_letterari + file, encoding='utf-8') as read_file:\n",
    "        sentences = [simple_preproc(k).lower().split() for k in read_file.readlines()]\n",
    "        with open(path_cade_periodi_letterari+'CADE_'+file, mode='w', encoding='utf-8') as f:\n",
    "            for s in sentences:\n",
    "                for w in s:\n",
    "                    f.write(w+\" \")\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80512995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alessandro Manzoni.txt', 'Dante Alighieri.txt', 'Dino Buzzati.txt', 'Francesco Petrarca.txt', \"Gabriele D'Annunzio.txt\", 'Giacomo Leopardi.txt', 'Giovanni Boccaccio.txt', 'Giuseppe Parini.txt', 'Italo Calvino.txt', 'Italo Svevo.txt', 'Ludovico Ariosto.txt', 'Luigi Pirandello.txt', 'Pier Paolo Pasolini.txt', 'Primo Levi.txt', 'Torquato Tasso.txt', 'Ugo Foscolo.txt', 'Umberto Eco.txt', 'Vittorio Alfieri.txt']\n"
     ]
    }
   ],
   "source": [
    "path_autori = 'C:/Users/cafe6/Università degli Studi di Milano-Bicocca/g.carbone8@campus.unimib.it - data/Preprocessing/autori/frasi_NON_lemmatizzate/'\n",
    "files_autori = [f for f in os.listdir(path_autori)]\n",
    "print(files_autori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34717b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cade_autori = 'C:/Users/cafe6/Università degli Studi di Milano-Bicocca/g.carbone8@campus.unimib.it - data/Preprocessing/autori/Corpus CADE/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec755791",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_autori:\n",
    "    with open(path_autori + file, encoding='utf-8') as read_file:\n",
    "        sentences = [simple_preproc(k).lower().split() for k in read_file.readlines()]\n",
    "        with open(path_cade_autori+'CADE_'+file, mode='w', encoding='utf-8') as f:\n",
    "            for s in sentences:\n",
    "                for w in s:\n",
    "                    f.write(w+\" \")\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5245eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
