{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cabb1b0",
   "metadata": {},
   "source": [
    "Carico tutte le librerie necessarie per stopwords, tokenizzazione e conteggi. La parte di position tag l'ho ripresa per l'inglese, ma non l'ho utilizzata nel nostro caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c63534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\giorg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\giorg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\giorg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\giorg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\giorg\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import sys\n",
    "import io \n",
    "import nltk\n",
    "\n",
    "# Stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('italian'))\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('italian')\n",
    "# Valutare di aggiungere eventuali forme arcariche delle stopwords italiane, o se c'è necessità di estendere questa lista\n",
    "  \n",
    "# Divido il testo in frasi in base ai punti\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Per il discorso lemmatizzazione dobbiamo valutare come muoverci con l'italiano\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Position tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Contatore parole uniche\n",
    "from collections import Counter\n",
    "\n",
    "# Per esplorare risultati\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d9d12",
   "metadata": {},
   "source": [
    "Valuto se la lista alternativa che ho trovato contiene più stopwords di quella di NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e1b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzare questo se necessario aggiungere stopwords da altre liste\n",
    "\n",
    "#other_stopwords = []\n",
    "#with open('ita_stopword.txt') as f:\n",
    "#    for line in f:\n",
    "#        other_stopwords.append(line)\n",
    "        \n",
    "#other_stopwords = [s.replace('\\n', '') for s in other_stopwords]\n",
    "\n",
    "# Unisco la lista di stopwords di nltk con quella estratta dal .txt\n",
    "#stopwords = list(nltk_stopwords)\n",
    "#stopwords.extend(x for x in other_stopwords if x not in stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78523ef7",
   "metadata": {},
   "source": [
    "Definisco le varie funzioni di preprocessing: senza lemmatizzazione e con lemmatizzazione, usando, nell'ordine, NLTK, Simplemma e SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a199f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (NO lemmatization)\n",
    "def preprocessing(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze,...\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frasi, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    filtered_sentence = [w for w in noalfa_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if filtered_sentence:\n",
    "      filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (CON lemmatization)\n",
    "\n",
    "def preprocessing_lem(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frasi, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Lemmatize\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in noalfa_sentence]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    filtered_sentence = [w for w in lemmatized_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if filtered_sentence:\n",
    "      filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34375a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplemma\n",
    "\n",
    "# Preprocessing (CON lemmatization w/ SIMPLEMMA)\n",
    "\n",
    "def preprocessing_simplemma(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frasi, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Lemmatize\n",
    "    lemmatized_sentence = [simplemma.lemmatize(w, lang='it') for w in noalfa_sentence]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    filtered_sentence = [w for w in lemmatized_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if filtered_sentence:\n",
    "      filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe75e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import it_core_news_lg\n",
    "\n",
    "nlp = it_core_news_lg.load()\n",
    "\n",
    "# Preprocessing (CON lemmatization w/ SPACY). In questo caso ho rimosso le stopwords prima della lemmatizzazione, va provato\n",
    "# con tutti a fare così\n",
    "\n",
    "def preprocessing_spacy(file_name):\n",
    "\n",
    "  output=\"\"\n",
    "  with open(file_name, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "          if not line.isspace(): # Rimuovo linee vuote\n",
    "              output+=line\n",
    "\n",
    "  # Divido il testo in frasi, basandomi sui punti\n",
    "  output_sentences = nltk.tokenize.sent_tokenize(output)\n",
    "\n",
    "  # Valutare se è necessario eliminare delle righe all'inizio o alla fine dei .txt se presentano licenze, ...\n",
    "  # output_sentences = output_sentences[:-]\n",
    "\n",
    "  filtered_sentences = []\n",
    "  # 'Pulisco' ogni frasi, una alla volta\n",
    "  for sentence in output_sentences:\n",
    "    # Metto tutto in lower case\n",
    "    lower_sentence=sentence.lower()\n",
    "    # Rimuovo caratteri non alfa numerici\n",
    "    noalfa_sentence = [w for w in word_tokenize(lower_sentence) if (w.isalpha()==True)]\n",
    "    # Rimuovo le stopwords e le parole di un solo carattere che potrebbero non essere incluse nella lista delle stopwords\n",
    "    nostop_sentence = [w for w in noalfa_sentence if ((w not in stops) and (len(w) > 1))]\n",
    "    # Lemmatize\n",
    "    lemmatized_sentence = \" \".join([w for w in nostop_sentence])\n",
    "    doc = nlp(lemmatized_sentence)\n",
    "    filtered_sentence = [w.lemma_ for w in doc]\n",
    "    # Ricostruisco la lista con le frasi 'pulite'\n",
    "    if filtered_sentence:\n",
    "      filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "  return filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bda79",
   "metadata": {},
   "source": [
    "Definisco la funzione per valutare i risultati del preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per esplorare i risultati\n",
    "def checkresults(sentences):\n",
    "  words = []\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "      words.append(sentences[i][j])\n",
    "\n",
    "  # Numero totale di parole\n",
    "  print('Parole: ' + str(len(words)))\n",
    "\n",
    "  # Numero di parole uniche\n",
    "  word_count = Counter(words)\n",
    "  keys = word_count.keys()\n",
    "  print('Parole uniche: '+ str(len(keys)))\n",
    "\n",
    "  # Parole più comuni\n",
    "  print('Parole più comuni:')\n",
    "  print(word_count.most_common(10))\n",
    "\n",
    "  # Stampo alcune frasi random\n",
    "  print('Frasi random:')\n",
    "  randomlist = random.sample(range(0, len(sentences)), 5)\n",
    "  for i in randomlist:\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savesentences(filename, sentences):\n",
    "  with open(filename + '.txt', 'w') as fp:\n",
    "    for sentence in sentences:\n",
    "      fp.write(str(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0621bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('Books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b418c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "frasi_decameron = preprocessing('Decameron.txt')\n",
    "frasi_decameron_lem = preprocessing_lem('Decameron.txt')\n",
    "frasi_decameron_simp = preprocessing_simplemma('Decameron.txt')\n",
    "frasi_decameron_spacy = preprocessing_spacy('Decameron.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkresults(frasi_decameron)\n",
    "checkresults(frasi_decameron_lem)\n",
    "checkresults(frasi_decameron_simp)\n",
    "checkresults(frasi_decameron_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Sentences')\n",
    "\n",
    "savesentences('frasi_decameron', frasi_decameron)\n",
    "savesentences('frasi_decameron_lem', frasi_decameron_lem)\n",
    "savesentences('frasi_decameron_simp', frasi_decameron_simp)\n",
    "savesentences('frasi_decameron_spacy', frasi_decameron_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2be5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frasi_decameron[0:1], frasi_decameron_lem[0:1], frasi_decameron_simp[0:1], frasi_decameron_spacy[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1caa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi ri-sposto in Books\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frasi_divina = preprocessing('Divina Commedia.txt')\n",
    "frasi_novelle = preprocessing('Novelle.txt')\n",
    "frasi_orlando = preprocessing('Orlando furioso.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Sentences')\n",
    "\n",
    "savesentences('frasi_divina', frasi_divina)\n",
    "savesentences('frasi_novelle', frasi_novelle)\n",
    "savesentences('frasi_orlando', frasi_orlando)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135bb21",
   "metadata": {},
   "source": [
    "### Topic modelling\n",
    "\n",
    "Si tratta di una cosa che potrebbe avere senso provare a fare mettendo insieme più libri, quindi costruendo i vari corpus in base all'autore, genere letterario o altro. Non ha senso farlo con le frasi di un singolo libro, in quanto, come si vede sotto, le parole più rilevanti risulterebbero quelle usate per costruire frasi di senso compiuto, come verbi o preposizioni. Invece, usando dei corpus che raccolgono più libri, sulla base delle statistiche della parole presenti in ciascun documento rispetto al totale, il modello riesce a individuare l'equilibrio fra i vari argomenti e valutare quali sono i più rilevanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b487f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decameron_sentences = open('decameron_frasi.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5540a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True rimuove la punteggiatura\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca939f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decameron_words = list(sent_to_words(decameron_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decameron_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Creo un dizionario\n",
    "id2word = corpora.Dictionary(decameron_words)\n",
    "# Creo un corpus\n",
    "texts = decameron_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa46962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Numero di topics\n",
    "num_topics = 10\n",
    "# Definisco il modello LDA\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Stampo le keyword associate ai topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "decameron_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5fb59",
   "metadata": {},
   "source": [
    "# TENTATIVO W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('Books/Sentences')#, togliere il commento se necessario\n",
    "#os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158350cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_W2V(sentences, book):\n",
    "  #addestro 2 modelli per libro, si può estendere tranquillamente il numero volendo\n",
    "\n",
    "  for it in range(2):\n",
    "    model = Word2Vec(sentences = sentences,\n",
    "                    #window = 5, default value\n",
    "                    min_count=10, #not consider word with absolute frequency <10 \n",
    "                    vector_size=300, #vector size \n",
    "                    sg = 1, #skipgram algorithm\n",
    "                    hs = 0,\n",
    "                    negative = 5, #negative sampling with 5 noise words\n",
    "                    workers = 5, #faster process\n",
    "                    epochs = 6 #6 iterations\n",
    "                    )\n",
    "  \n",
    "    model.save(book.lower() + \"_\" + str(it) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c239cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = {'Decameron.txt' : 'Decameron', 'Divina Commedia.txt' : 'Divina Commedia', 'Novelle.txt' : 'Novelle',\n",
    "             'Orlando furioso.txt' : 'Orlando Furioso'}\n",
    "\n",
    "for book in book_dict.keys():\n",
    "    sentences = preprocessing(book)\n",
    "    training_W2V(sentences, book_dict[book])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2cc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "divina0_model = Word2Vec.load(\"divina commedia_0.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0664aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "divina0_model.wv.most_similar(\"amore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026d05a",
   "metadata": {},
   "source": [
    "# TENTATIVO CADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7334c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cade.cade import CADE\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1a08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('Books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b5b9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_deca = preprocessing('Decameron.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "701f8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_novelle = preprocessing('Novelle.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed6ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentences/prova_deca_CADE.txt', mode='w') as f:\n",
    "    for s in sentences_deca:\n",
    "        for w in s:\n",
    "            f.write(w+\" \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7ba6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sentences/prova_novelle_CADE.txt', mode='w') as f:\n",
    "    for s in sentences_novelle:\n",
    "        for w in s:\n",
    "            f.write(w+\" \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057750ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906e654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo il compass (non mi funziona !cat su notebook)\n",
    "\n",
    "with open('prova_compass.txt', 'w') as outFile:\n",
    "    with open('prova_deca_CADE.txt', 'r') as deca, open('prova_novelle_CADE.txt', 'r') as novelle:\n",
    "        outFile.write(deca.read())\n",
    "        outFile.write(novelle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "284acda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = CADE(size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1838b2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass from scratch.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maligner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_compass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprova_compass.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\cade\\cade.py:121\u001b[0m, in \u001b[0;36mCADE.train_compass\u001b[1;34m(self, compass_text, overwrite, save)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compass_exists:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent saved compass will be overwritten after training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompass\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompass.model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\cade\\cade.py:101\u001b[0m, in \u001b[0;36mCADE.train_model\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     99\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompass \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 101\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     model\u001b[38;5;241m.\u001b[39mbuild_vocab(sentences, trim_rule\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_trimming_rule \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompass \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompass \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "aligner.train_compass(\"prova_compass.txt\") ### NON SO COME RISOLVERE QUESTO ERRORE, SU COLAB SEMBRA FUNZIONARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_one = aligner.train_slice(\"prova_deca_CADE.txt\", save=True)\n",
    "slice_two = aligner.train_slice(\"prova_novelle_CADE.txt\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86440c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deca = Word2Vec.load('model/prova_deca_CADE.model')\n",
    "model_novelle = Word2Vec.load('model/prova_novelle_CADE.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deca.wv.most_similar(\"donna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_novelle.wv.most_similar(\"donna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5816da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lauretta = model_deca.wv['lauretta']\n",
    "model_novelle.wv.most_similar(positive=[lauretta])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba18286d27dd3f1705df4025c31d2b7021c67fcd01f023a6f194ed0db21ec453"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
